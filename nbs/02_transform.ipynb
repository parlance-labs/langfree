{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a573de2-34fe-4fdc-a5c7-7018318f167a",
   "metadata": {},
   "source": [
    "# transform\n",
    "\n",
    "> common transformations for LLM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1851b3f-5b38-4d50-8f61-c3fe0d54f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d65a5-c571-4341-b690-39ca2721fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os, copy, json\n",
    "import openai, langsmith\n",
    "from typing import List, Callable\n",
    "from random import shuffle\n",
    "from collections import defaultdict\n",
    "\n",
    "from langfree.runs import _temp_env_var, Client, _ischatopenai\n",
    "from pydantic import BaseModel\n",
    "from langchain.adapters import openai as adapt\n",
    "from langchain.load import load\n",
    "from fastcore.foundation import L\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea68e53-44b5-4cf5-876b-aead27071d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b0131-7055-418b-9263-69180553f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1248be-3d29-434e-9fc2-7e344d37d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def chat(**kwargs):\n",
    "    \"A wrapper around `openai.ChatCompletion` that has automatic retries.\" \n",
    "    client.api_key = os.environ['OPENAI_API_KEY']\n",
    "    return client.chat.completions.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991cdff3-e099-4e70-b9f9-1fb1b169bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rephrase(sentence, max_tokens=100, temperature=0.95):\n",
    "    \"Rephrase a sentence. Useful for data augmentation for finetuning.\"\n",
    "    client.api_key = os.environ['OPENAI_API_KEY']\n",
    "    response = chat(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        model=\"gpt-4\", \n",
    "        messages=[\n",
    "         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "         {\"role\": \"user\", \"content\": f\"Rephrase the following sentence in one short sentence: {sentence}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4a3ed-16f0-4ed4-ae93-f8668c786b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose an email to example@gmail.com inquiring about rescheduling the appointment to Friday.\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "_phrase = 'Write an email to example@gmail.com asking if we can move the appointment to friday'\n",
    "print(rephrase(_phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b23b2-40f2-402b-84c8-682bde5adc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _gen_name():\n",
    "    \"Generate a random name\"\n",
    "    client.api_key = os.environ['OPENAI_API_KEY']\n",
    "    response = chat(\n",
    "        temperature=1.9,\n",
    "        max_tokens=4,\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[\n",
    "         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "         {\"role\": \"user\", \"content\": f\"Imagine a full name for a person. Only return a first and last name.\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip().replace('.', '')\n",
    "\n",
    "def gen_name():\n",
    "    \"Generate a random name, usefule for data augmentation and privacy.\"\n",
    "    while True:\n",
    "        nm = _gen_name()\n",
    "        if len(nm) <= 18:\n",
    "            return nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599f396-73b6-4a89-941f-c4682ee52627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexis Chang\n",
      "Ann Douglais\n",
      "Camilla Castillo\n"
     ]
    }
   ],
   "source": [
    "#|eval:false\n",
    "for i in range(3):\n",
    "    print(gen_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bfb23-8774-46c0-8865-97c53c59dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "tmp_env = {'LANGCHAIN_API_KEY': os.environ['LANGCHAIN_API_KEY_PUB'], 'LANGSMITH_PROJECT_ID': os.environ['LANGCHAIN_PROJECT_ID_PUB']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc66954-70fa-4022-bd88-085722bcfa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "_tst_run_id = '59080971-8786-4849-be88-898d3ffc2b45'\n",
    "client = langsmith.Client()\n",
    "run = client.read_run(_tst_run_id)\n",
    "msg = run.outputs['generations'][0]['message']\n",
    "assert load(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663a851-03a2-4f3d-8c58-2d5cc613ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def fetch_run_componets(run_id:str):\n",
    "    \"Return the `inputs`, `output` and `funcs` for a run of type `ChatOpenAI`.\"\n",
    "    client = langsmith.Client()\n",
    "    run = client.read_run(run_id)\n",
    "    _ischatopenai(run)\n",
    "    output = adapt.convert_message_to_dict(load(run.outputs['generations'][0]['message']))\n",
    "    inputs = [adapt.convert_message_to_dict(load(m)) for m in run.inputs['messages']]\n",
    "    params = run.extra['invocation_params']\n",
    "    \n",
    "    for inp in inputs:\n",
    "        if 'function_call' in inp and inp.get('content', None) is None:\n",
    "            del inp['content']\n",
    "    funcs = params.get(\"functions\", [])\n",
    "    return inputs, output, funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77caa6fd-deac-45e1-ba1b-aad781331075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first input:\n",
      "{'role': 'system', 'content': \"You are a helpful documentation Q&A assistant, trained to answer questions from LangSmith's documentation. LangChain is a framework for building applications using large language models.\\nThe current time is 2023-09-05 16:49:07.308007.\\n\\nRelevant documents will be retrieved in the following messages.\"} \n",
      "\n",
      "output:\n",
      "{'role': 'assistant', 'content': \"Currently, LangSmith does not support project migration between organizations. However, you can manually imitate this process by reading and writing runs and datasets using the SDK. Here's an example of exporting runs:\\n\\n1. Read the runs from the source organization using the SDK.\\n2. Write the runs to the destination organization using the SDK.\\n\\nBy following this process, you can transfer your runs from one organization to another. However, it may be faster to create a new project within your destination organization and start fresh.\\n\\nIf you have any further questions or need assistance, please reach out to us at support@langchain.dev.\"} \n",
      "\n",
      "functions:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "_tst_run_id = '1863d76e-1462-489a-a8a7-e0404239fe47'\n",
    "\n",
    "with _temp_env_var(tmp_env):  #context manager that has specific environment vars for testing                    \n",
    "    _inp, _out, _funcs = fetch_run_componets(_tst_run_id)\n",
    "\n",
    "print(f\"\"\"first input:\n",
    "{_inp[0]} \n",
    "\n",
    "output:\n",
    "{_out} \n",
    "\n",
    "functions:\n",
    "{_funcs}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955d7b5-491f-4f93-9a0c-9cdbcba55000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "_run_id = '59080971-8786-4849-be88-898d3ffc2b45'\n",
    "_inputs, _output, _funcs = fetch_run_componets(_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3deda0-b415-4683-9f5b-fc340f80c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "class RunData(BaseModel):\n",
    "    \"Key components of a run from LangSmith\"\n",
    "    inputs:List[dict]\n",
    "    output:dict\n",
    "    funcs:List[dict] \n",
    "    run_id:str\n",
    "\n",
    "    @classmethod\n",
    "    def from_run_id(cls, run_id:str):\n",
    "        \"Create a `RunData` object from a run id.\"\n",
    "        inputs, output, funcs = fetch_run_componets(run_id)\n",
    "        return cls(inputs=inputs, output=output, funcs=funcs, run_id=run_id)\n",
    "\n",
    "    def to_msg_dict(self):\n",
    "        \"Transform the instance into a dict in the format that can be used for OpenAI fine-tuning.\"\n",
    "        msgs = self.inputs + [self.output]\n",
    "        return {\"functions\": self.funcs,\n",
    "                \"messages\": msgs}\n",
    "\n",
    "    def to_json(self):\n",
    "        \"The json version of `to_msg_dict`.\"\n",
    "        return json.dumps(self.to_msg_dict())\n",
    "\n",
    "    @property\n",
    "    def flat_input(self):\n",
    "        \"The input to the LLM in markdown.\"\n",
    "        return self._flatten_data(self.inputs)\n",
    "\n",
    "    @property\n",
    "    def flat_output(self):\n",
    "        \"The output of the LLM in markdown.\"\n",
    "        return self._flatten_data([self.output])\n",
    "\n",
    "    @classmethod\t\n",
    "    def _flatten_data(cls, data):\n",
    "        \"Produce a flattened view of the data as human readable Markdown.\"\n",
    "        md_str = \"\"\n",
    "        for item in data:\n",
    "            # Heading\n",
    "            role = item['role']\n",
    "            if role == 'assistant' and 'function_call' in item:\n",
    "                role += ' - function call'\n",
    "            if role == 'function':\n",
    "                role += ' - results'\n",
    "            \n",
    "            md_str += f\"### {role.title()}\\n\\n\"\n",
    "\n",
    "            content = item.get('content', '')\n",
    "            if content: md_str += content + \"\\n\"\n",
    "                \n",
    "            elif 'function_call' in item:\n",
    "                func_name = item['function_call']['name']\n",
    "                args = json.loads(item['function_call']['arguments'])\n",
    "                formatted_args = ', '.join([f\"{k}={v}\" for k, v in args.items()])\n",
    "                md_str += f\"{func_name}({formatted_args})\\n\"\n",
    "            md_str += \"\\n\"\n",
    "        return md_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf0eab-38c7-44b2-b287-c941ddba299c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L98){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.from_run_id\n",
       "\n",
       ">      RunData.from_run_id (run_id:str)\n",
       "\n",
       "Create a `RunData` object from a run id."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L98){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.from_run_id\n",
       "\n",
       ">      RunData.from_run_id (run_id:str)\n",
       "\n",
       "Create a `RunData` object from a run id."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RunData.from_run_id, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f6788-82a4-485c-8473-45c19d43e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1863d76e-1462-489a-a8a7-e0404239fe47 has 3 inputs.\n",
      "Run 1863d76e-1462-489a-a8a7-e0404239fe47 output:\n",
      "{'role': 'assistant', 'content': \"Currently, LangSmith does not support project migration between organizations. However, you can manually imitate this process by reading and writing runs and datasets using the SDK. Here's an example of exporting runs:\\n\\n1. Read the runs from the source organization using the SDK.\\n2. Write the runs to the destination organization using the SDK.\\n\\nBy following this process, you can transfer your runs from one organization to another. However, it may be faster to create a new project within your destination organization and start fresh.\\n\\nIf you have any further questions or need assistance, please reach out to us at support@langchain.dev.\"}\n"
     ]
    }
   ],
   "source": [
    "with _temp_env_var(tmp_env): #context manager that has specific environment vars for testing\n",
    "    rd = RunData.from_run_id(_tst_run_id)\n",
    "\n",
    "print(f'Run {rd.run_id} has {len(rd.inputs)} inputs.')\n",
    "print(f'Run {rd.run_id} output:\\n{rd.output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72979269-83ad-468b-81f1-bbda71a27cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L103){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.to_msg_dict\n",
       "\n",
       ">      RunData.to_msg_dict ()\n",
       "\n",
       "Transform the instance into a dict in the format that can be used for OpenAI fine-tuning."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L103){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.to_msg_dict\n",
       "\n",
       ">      RunData.to_msg_dict ()\n",
       "\n",
       "Transform the instance into a dict in the format that can be used for OpenAI fine-tuning."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RunData.to_msg_dict, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add34097-4946-4ad9-bd28-1e5c06734cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'How do I move my project between organizations?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Currently, LangSmith does not support project migration between organizations. However, you can manually imitate this process by reading and writing runs and datasets using the SDK. Here's an example of exporting runs:\\n\\n1. Read the runs from the source organization using the SDK.\\n2. Write the runs to the destination organization using the SDK.\\n\\nBy following this process, you can transfer your runs from one organization to another. However, it may be faster to create a new project within your destination organization and start fresh.\\n\\nIf you have any further questions or need assistance, please reach out to us at support@langchain.dev.\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.to_msg_dict()['messages'][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d5fa0-4265-461c-a241-fdca3d0e51dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L109){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.to_json\n",
       "\n",
       ">      RunData.to_json ()\n",
       "\n",
       "The json version of `to_msg_dict`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L109){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.to_json\n",
       "\n",
       ">      RunData.to_json ()\n",
       "\n",
       "The json version of `to_msg_dict`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RunData.to_json, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb8e59-37cc-4a37-bb1d-564219b7b51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"functions\": [], \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful documentation Q&A as'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.to_json()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d314a9-7ced-40ea-9d2e-2628d6cefebe",
   "metadata": {},
   "source": [
    "The properties `flat_input` and `flat_output` allow you to view the input to the LLM and the output in a human readable format (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15e8a1-b173-4e94-935d-084cee1ea9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L114){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.flat_input\n",
       "\n",
       ">      RunData.flat_input ()\n",
       "\n",
       "The input to the LLM in markdown."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L114){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.flat_input\n",
       "\n",
       ">      RunData.flat_input ()\n",
       "\n",
       "The input to the LLM in markdown."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RunData.flat_input, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7abc8-3491-440b-9541-9ba362f5b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System\n",
      "\n",
      "You are a helpful documentation Q&A assistant, trained to answer questions from LangSmith's documentation. LangChain is a framework for building applications using large language models.\n",
      "The current time is 2023-09-05 16:49:07.308007.\n",
      "\n",
      "Relevant documents will be retrieved in the following messages.\n",
      "\n",
      "### System\n",
      "\n",
      "\n",
      "\n",
      "Skip to main content\n",
      "\n",
      " **🦜️🛠️ LangSmith Docs**Python DocsJS/TS Docs\n",
      "\n",
      "Sear\n"
     ]
    }
   ],
   "source": [
    "print(rd.flat_input[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c56d8-3a02-47fa-8b3e-16d530c59c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.flat_output\n",
       "\n",
       ">      RunData.flat_output ()\n",
       "\n",
       "The output of the LLM in markdown."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/parlance-labs/langfree/blob/main/langfree/transform.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### RunData.flat_output\n",
       "\n",
       ">      RunData.flat_output ()\n",
       "\n",
       "The output of the LLM in markdown."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RunData.flat_output, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35043ba-c21e-47e4-8339-43f42e6344ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Assistant\n",
      "\n",
      "Currently, LangSmith does not support project migration between organizations. However, you can manually imitate this process by reading and writing runs and datasets using the SDK. Here's an example of exporting runs:\n",
      "\n",
      "1. Read the runs from the source organization using the SDK.\n",
      "2. Write the runs to the destination organization using the SDK.\n",
      "\n",
      "By following this process, you can transfer your runs from one organization to another. However, it may be faster to create a new project within your destination organization and start fresh.\n",
      "\n",
      "If you have any further questions or need assistance, please reach out to us at support@langchain.dev.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rd.flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe02728-3a3c-4008-8e41-34102078bd83",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "`tsfm_nm_rephrase` does the following transformations to runs:\n",
    "\n",
    "- Substitutes a random name in various parts chat conversation in a consistent way (data augmentation)\n",
    "- Rephrases the human input (data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3a71b-3bac-48e9-90be-3b739acd46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _sub_name_in_func(funcs, name):\n",
    "    \"Substitute 'Unit Test' for `name` in the `email-campaign-creator` function\"\n",
    "    emailfunc = L(funcs).filter(lambda x: x['name'] == 'email-campaign-creator')\n",
    "    if emailfunc:\n",
    "        func = emailfunc[0]\n",
    "        desc = func['parameters']['properties']['body']['description']\n",
    "        new_desc = desc.replace('Unit Test', name)\n",
    "        func['parameters']['properties']['body']['description'] = new_desc\n",
    "    return funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbd8e4-7604-40d0-8cf6-e59e901e742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "_testfuncs = _sub_name_in_func(_funcs, name='Hamel Husain')\n",
    "assert 'Hamel' in L(_testfuncs).filter(lambda x: x['name'] == 'email-campaign-creator')[0]['parameters']['properties']['body']['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496fc492-6c7c-4c1d-8bdc-66aa99d9591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _sub_name_in_output(output, name):\n",
    "    \"Subtitute `[Your Name]` with `name` in the output.\"\n",
    "    output['content'] = output['content'].replace('[Your Name]', name)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655e9d4-9f9a-402f-9bac-045186ba35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "_out = _sub_name_in_output(output=_output, name='Hamel')\n",
    "assert 'Hamel' in _out['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3de33-8304-4019-b417-ade208fb1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reword_input(inputs):\n",
    "    \"Rephrase the first human input.\"\n",
    "    copy_inputs = copy.deepcopy(inputs)\n",
    "    for inp in copy_inputs:\n",
    "        if inp['role'] == 'user': \n",
    "            inp['content'] = rephrase(inp['content'])\n",
    "            print(f\"rephrased input as: {inp['content']}\")\n",
    "            break\n",
    "    return copy_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81cc24-08bf-4c00-885b-679f8497614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval:false\n",
    "_tst_inp = reword_input(_inputs)\n",
    "assert _tst_inp[1]['content'] != _inputs[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2807ca-3f61-469a-8d44-bb1ec4c2805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def tsfm_nm_rephrase(rundata:RunData, name=None) -> RunData:\n",
    "    \"Substitutes names in functions & outputs and rephrases the language model input.\"\n",
    "    if name is None: name=gen_name()                    # generates a random name to be used to substitute a boilerplate name\n",
    "    print(f'Substituting name: {name}')\n",
    "    inputs = reword_input(rundata.inputs)              # rephrases the input to the language model\n",
    "    output = _sub_name_in_output(rundata.output, name)  # substitutes the template `[Your Name]` with `name` in the output of the language model\n",
    "    funcs = _sub_name_in_func(rundata.funcs, name)      # substitutes the template `[Your Name]` with `name` in the a function schema description\n",
    "    return RunData(inputs=inputs, output=output, funcs=funcs, run_id=rundata.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbd04d-6f15-4ed9-ab02-92d949dcbfa9",
   "metadata": {},
   "source": [
    "In the below example, `[Your name]` is being substituted with an actual name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e6e09e-fa0e-4f8c-a9bd-6536a191a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _rundata = RunData.from_run_id(_run_id)\n",
    "# _orig = _rundata.to_msg_dict()\n",
    "# _tsfm = tsfm_nm_rephrase(_rundata).to_msg_dict()\n",
    "\n",
    "# assert 'Your Name' not in _tsfm['messages'][-1]  # verify that `[Your Name]` is not present\n",
    "# assert _orig['messages'][1]['content'] != _tsfm['messages'][1]['content']  # make sure the message is different afer substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f8d64-8421-4361-998a-296609dc0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(tsfm_nm_rephrase);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a1b22-b51c-4ae5-a2d5-6ab935e9e1ee",
   "metadata": {},
   "source": [
    "## Preparing `.jsonl` files\n",
    "\n",
    "[OpenAI fine-tuning](https://platform.openai.com/docs/guides/fine-tuning) takes `.jsonl` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6ce23-c8bb-47c4-b4b8-314666f82021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def write_to_jsonl(data_list:List[RunData], filename:str):\n",
    "    \"\"\"\n",
    "    Writes a list of dictionaries to a .jsonl file.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_list (list of `RunData`): The data to be written.\n",
    "    - filename (str): The name of the output file.\n",
    "    \"\"\"\n",
    "    shuffle(data_list)\n",
    "    with open(filename, 'w') as f:\n",
    "        for entry in data_list:\n",
    "            f.write(f\"{entry.to_json()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144d12d-3f90-471d-a503-045832c9d552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substituting name: Jake Wilson\n",
      "rephrased input as: Implement email marketing for 2430 Victory Park Lane, Dallas TX.\n",
      "Substituting name: Kimberly Ramirez\n",
      "rephrased input as: Arrange a team-building event for the upcoming month.\n"
     ]
    }
   ],
   "source": [
    "#|eval:false\n",
    "_rids = ['59080971-8786-4849-be88-898d3ffc2b45', '8cd7deed-9547-4a07-ac01-55e9513ca1cd']\n",
    "_tsfm_runs = [tsfm_nm_rephrase(RunData.from_run_id(rid)) for rid in _rids]\n",
    "write_to_jsonl(_tsfm_runs, '_data/test_data.jsonl');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b3cd8-f7f7-4b24-90de-10505c118be8",
   "metadata": {},
   "source": [
    "It can save you time to validate jsonl files prior to uploading them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a60ed0-a3c9-4220-95f7-bb168fb650ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def validate_jsonl(fname):\n",
    "    \"Code is modified from https://cookbook.openai.com/examples/chat_finetuning_data_prep, but updated for function calling.\"\n",
    "    # Load the dataset\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    # Initial dataset stats\n",
    "    print(\"Num examples:\", len(dataset))\n",
    "        \n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    for i, ex in enumerate(dataset):\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        for im, message in enumerate(messages):\n",
    "            if \"role\" not in message or (\"content\" not in message and 'function_call' not in message):\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "                print(f'message_unrecognized_key {[k for k in message.keys() if k not in [\"role\", \"content\", \"name\"]]} in row:{i} message {im}')\n",
    "\n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "                print(f'unrecognized_role {message.get(\"role\", None)} in row:{i} message {im}')\n",
    "\n",
    "            content = message.get(\"content\", None)\n",
    "            if (not content or not isinstance(content, str)) and 'function_call' not in message:\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "                print(f'missing_content in row:{i} message {im}')\n",
    "\n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f8f01-1787-401b-9db7-d0657ec6af85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 2\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "validate_jsonl('_data/test_data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90662f53-de19-4032-a9ac-80fdacb78509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc86a80-b68f-4ed5-9bbf-790b9dbe92b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
